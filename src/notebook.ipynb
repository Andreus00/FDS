{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: ReID\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We wanted to reidentify a person, given some samples of the dataset. The idea is to learn the main features of the person in order to distinguish him/her from other people.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we did was to import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "from load_data import DataLoader\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, Normalizer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import config\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from utils import filter_and_split_dataset, filter_faces\n",
    "from sklearn.metrics import DetCurveDisplay, RocCurveDisplay, ConfusionMatrixDisplay, roc_auc_score, roc_curve, det_curve, confusion_matrix, average_precision_score\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataLoader()\n",
    "d.read_dataset()\n",
    "d.shuffle_videos()\n",
    "d.display_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do was to design the learning setting. The first and easiest idea was to test a variety of classifiers, and see which works better for our features.\n",
    "\n",
    "We tested different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name, model_skel, model_clothes, model_face, X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test):\n",
    "    model_skel.fit(X_skel, y_train)\n",
    "    model_clothes.fit(X_clothes, y_train)\n",
    "    model_face.fit(X_face, y_face)\n",
    "\n",
    "\n",
    "    print(\"-\" * 10)\n",
    "    print(\"|\", name, \"|\")\n",
    "    # Test\n",
    "    print(\"-\" * 10)\n",
    "    skel_pred = model_skel.predict(X_skel_test)\n",
    "    print(classification_report(y_test, skel_pred))\n",
    "    print(\"-\" * 10)\n",
    "    clothes_pred = model_clothes.predict(X_clothes_test)\n",
    "    print(classification_report(y_test, clothes_pred))\n",
    "    print(\"-\" * 10)\n",
    "    face_pred = model_face.predict(X_face_test)\n",
    "    print(classification_report(y_face_test, face_pred))\n",
    "\n",
    "    skel_pred_roc = model_skel.predict_proba(X_skel_test)\n",
    "    clothes_pred_roc = model_clothes.predict_proba(X_clothes_test)\n",
    "    face_pred_roc = model_face.predict_proba(X_face_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(config.SAMPLED_PATH + \"video_1_sample_0_X.npy\")\n",
    "y = np.load(config.SAMPLED_PATH + \"video_1_sample_0_y.npy\")\n",
    "X_skel, X_clothes, X_face, _, ___, ____, y_train, _____, y_face, ______ = filter_and_split_dataset(X, y)\n",
    "\n",
    "X_2 = np.load(config.SAMPLED_PATH + \"video_1_sample_1_X.npy\")\n",
    "y_2 = np.load(config.SAMPLED_PATH + \"video_1_sample_1_y.npy\")\n",
    "X_skel_test, X_clothes_test, X_face_test, _, __, ___, y_test, ____, y_face_test, _____ = filter_and_split_dataset(X_2, y_2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Skeleton\n",
    "skel_feature = np.asarray([\"chest\", \"right_upper_arm\", \"right_lower_arm\", \"left_upper_arm\", \"left_lower_arm\", \"right_upper_leg\", \"right_lower_leg\", \"left_upper_leg\", \"left_lower_leg\"])\n",
    "rf = RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200, random_state = 42)\n",
    "rf.fit(X_skel, y_train)\n",
    "rf.feature_importances_\n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "ax[0].barh(skel_feature[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "# Clothes\n",
    "clothes_feature = np.asarray([str(i) for i in range(0, 40)])\n",
    "rf = RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 10, n_estimators = 100, random_state = 4)\n",
    "rf.fit(X_clothes, y_train)\n",
    "rf.feature_importances_\n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "ax[1].barh(clothes_feature[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "\n",
    "# Face\n",
    "face = np.asarray([\"eyes_lenght\", \"eyes_width\", \"nose_heigth\", \"nose_width\", \"mouth_width\", \"mouth_height\", \"chin\", \"face_width\", \"face_height\", \"distance_nose_mouth\", \"distance_mouth_chin\", \"distance_between_eyes\"])\n",
    "rf = RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200, random_state = 42)\n",
    "rf.fit(X_face, y_face)\n",
    "rf.feature_importances_\n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "ax[2].barh(face[sorted_idx], rf.feature_importances_[sorted_idx])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers_skel = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=2, min_samples_split=10, random_state=2, splitter='random'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200, random_state = 42),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=3, p=1, weights='distance'),\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=10000),\n",
    "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
    "    # \"SVC\": SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42),\n",
    "}\n",
    "\n",
    "classifiers_clothes = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=1, min_samples_split=10, random_state=2, splitter='best'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 10, n_estimators = 100, random_state = 4),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=7, p=1, weights='distance'),\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=10000),\n",
    "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
    "    # \"SVC\": SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42),\n",
    "}\n",
    "\n",
    "classifiers_face = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=2, min_samples_split=5, random_state=2, splitter='random'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 50, random_state = 42),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=9, p=1, weights='distance'),\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=10000),\n",
    "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
    "    # \"SVC\": SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42),\n",
    "}\n",
    "\n",
    "fig, ((ax_roc_skel, ax_roc_clothes, ax_roc_face), (ax_det_skel, ax_det_clothes, ax_det_face)) = plt.subplots(2, 3, figsize =(20, 10))\n",
    "\n",
    "files = [f for f in os.listdir(\"./sampled\") if re.match(\"(video_)([1-9]*)(_sample_)([0-9]*)_([Xy])(.npy)\", f)]\n",
    "files.sort()\n",
    "samples = [(files[i], files[i+1], files[i+2], files[i+3]) for i in range(0, len(files) - 2, 4)]\n",
    "\n",
    "def fit_dataset(model, X_train, y_train, X_test, y_test):    \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    res = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, res)\n",
    "    auc = roc_auc_score(y_test, res)\n",
    "\n",
    "    fpr2, tpr2, thresholds2 = det_curve(y_test, res)\n",
    "\n",
    "    return fpr, tpr, auc, fpr2, tpr2\n",
    "\n",
    "def plot_roc_det_curves(name, model, ax_roc : plt.Axes, ax_det : plt.Axes, samples, modality):\n",
    "\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    fprs2 = []\n",
    "    tprs2 = []\n",
    "    roc_aucs = []\n",
    "\n",
    "    # fit the model for each training sample and average the fpr, tpr and roc_auc\n",
    "    for f_X, f_y, f_X_2, f_y_2 in samples:\n",
    "\n",
    "        X_train = np.load(\"./sampled/\" + f_X)\n",
    "        y_train = np.load(\"./sampled/\" + f_y)\n",
    "\n",
    "        X_test = np.load(\"./sampled/\" + f_X_2)\n",
    "        y_test = np.load(\"./sampled/\" + f_y_2)\n",
    "\n",
    "        if modality==\"face\":\n",
    "            features_face_not_filtered = X_train[:, 9 + config.NUM_POINTS_LBP:]\n",
    "            X_train, y_train = filter_faces(features_face_not_filtered, y_train)\n",
    "            features_face_not_filtered = X_test[:, 9 + config.NUM_POINTS_LBP:]\n",
    "            X_test, y_test = filter_faces(features_face_not_filtered, y_test)\n",
    "        elif modality==\"skel\":\n",
    "            X_train = X_train[:, 0:9]\n",
    "            X_test = X_test[:, 0:9]\n",
    "        elif modality==\"clothes\":\n",
    "            X_train =  X_train[:, 9:9 + config.NUM_POINTS_LBP]\n",
    "            X_test = X_test[:, 9:9+config.NUM_POINTS_LBP]\n",
    "        else:\n",
    "            raise Exception(\"Modality not supported\")\n",
    "\n",
    "        fpr, tpr, roc_auc, fpr2, tpr2 = fit_dataset(model, X_train, y_train, X_test, y_test)\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        fprs2.append(fpr2)\n",
    "        tprs2.append(tpr2)\n",
    "        roc_aucs.append(roc_auc)\n",
    "\n",
    "    longest_fpr = len(max(fprs, key=len))\n",
    "    longest_tpr = len(max(tprs, key=len))\n",
    "    longest_fpr2 = len(max(fprs2, key=len))\n",
    "    longest_tpr2 = len(max(tprs2, key=len))\n",
    "\n",
    "    # pad the shorter arrays with the last value\n",
    "    for i in range(0, len(fprs)):\n",
    "        fprs[i] = np.pad(fprs[i], (0, longest_fpr - len(fprs[i])), 'edge')\n",
    "        tprs[i] = np.pad(tprs[i], (0, longest_tpr - len(tprs[i])), 'edge')\n",
    "    \n",
    "    for i in range(0, len(fprs2)):\n",
    "        fprs2[i] = np.pad(fprs2[i], (0, longest_fpr2 - len(fprs2[i])), 'edge')\n",
    "        tprs2[i] = np.pad(tprs2[i], (0, longest_tpr2 - len(tprs2[i])), 'edge')\n",
    "    \n",
    "    # average the fpr and tpr and calculate the auc\n",
    "    fpr = np.mean(fprs, axis=0)\n",
    "    tpr = np.mean(tprs, axis=0)\n",
    "    auc = np.mean(roc_aucs)\n",
    "    \n",
    "\n",
    "    fpr2 = np.mean(fprs2, axis=0)\n",
    "    tpr2 = np.mean(tprs2, axis=0)\n",
    "\n",
    "    ax_roc.plot(fpr, tpr, label=name + \" (AUC = \" + str(round(auc, 2)) + \")\")\n",
    "    ax_det.plot(fpr2, tpr2, label=name)\n",
    "\n",
    "    ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "    ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "    ax_roc.set_title(f\"ROC Curve - {modality}\")\n",
    "    ax_det.set_xlabel(\"Detection Error Rate\")\n",
    "    ax_det.set_ylabel(\"Detection Rate\")\n",
    "    ax_det.set_title(f\"DET Curve - {modality}\")\n",
    "    ax_roc.legend(loc=\"lower right\")\n",
    "    ax_det.legend(loc=\"upper right\")\n",
    "    ax_roc.grid(linestyle=\"--\")\n",
    "    ax_det.grid(linestyle=\"--\")\n",
    "\n",
    "\n",
    "for name, model in classifiers_skel.items():\n",
    "\n",
    "    plot_roc_det_curves(name, model, ax_roc_skel, ax_det_skel, samples, 'skel')\n",
    "\n",
    "for name, model in classifiers_clothes.items():\n",
    "    \n",
    "    plot_roc_det_curves(name, model, ax_roc_clothes, ax_det_clothes, samples, \"clothes\")   \n",
    "\n",
    "for name, model in classifiers_face.items():\n",
    "        \n",
    "    plot_roc_det_curves(name, model, ax_roc_face, ax_det_face, samples, \"face\") \n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"ROC_DET_curves.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class_names = ['Different', 'Same']\n",
    "\n",
    "def fit_models(models, X_train, y_train):\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train.ravel())\n",
    "        # print(name, 'trained.')\n",
    "    return\n",
    "\n",
    "def evaluate_models(models,model_name,X_test, y_test,printable):\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        if printable:\n",
    "            print(name)\n",
    "\n",
    "        cv_accuracy = cross_val_score(model, X_test, y_test.ravel(), n_jobs=-1, scoring='accuracy')\n",
    "        cv_f1_macro = cross_val_score(model, X_test, y_test.ravel(), n_jobs=-1, scoring='f1_macro')\n",
    "        # print(cross_val_score(model, X_test, y_test, scoring='accuracy'))\n",
    "        if printable:\n",
    "            print(classification_report(y_test, y_pred,zero_division=0))\n",
    "            print(\"Accuracy: %0.4f (+/- %0.4f)\" % (cv_accuracy.mean(), cv_accuracy.std() * 2))\n",
    "        # print(cross_val_score(model, X_test, y_test, scoring='f1_macro'))\n",
    "            print(\"f1-score: %0.4f (+/- %0.4f)\" % (cv_f1_macro.mean(), cv_f1_macro.std() * 2))\n",
    "        if name.count(\"accuracy\")> 0:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        if printable:\n",
    "            print('------------------------------------')\n",
    "    return cmn\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "best_param_table = PrettyTable()\n",
    "def tune_model(part,model, param_grid, scoring, x_train, y_train, grid_jobs):\n",
    "    # print('tuning...')\n",
    "    clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, verbose=0, n_jobs=grid_jobs)\n",
    "    clf.fit(x_train, y_train.ravel())\n",
    "    # print('done')\n",
    "    # print(\"%s - %s - %s: Best parameter  %s\" % (model,part, scoring,clf.best_params_))\n",
    "    best_params = clf.best_params_.copy()\n",
    "    row = [part,scoring] + list(best_params.values()) + list([round(clf.best_score_ ,2)])\n",
    "    best_param_table.add_row(row)\n",
    "    return best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SVM_param_grid = {\n",
    "    'C': [0.5, 1],\n",
    "    'degree': [1,2,3],\n",
    "    'gamma': ['scale'],\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid', 'linear'],\n",
    "    'random_state': [2,42]\n",
    "}\n",
    "\n",
    "RF_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'random_state': [2,42]\n",
    "}\n",
    "\n",
    "\n",
    "Gauss_param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "KNN_param_grid = {\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40],\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'p': [1, 2],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "\n",
    "}\n",
    "\n",
    "DT_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'random_state': [2,42],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "Regres_param_grid = {\n",
    "    'C': [0.5,1,4],\n",
    "    'max_iter': [10000],\n",
    "    'random_state': [2,42],\n",
    "    'solver': ['lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"KNN\": (KNeighborsClassifier, KNN_param_grid),\n",
    "    \"DT\": (DecisionTreeClassifier, DT_param_grid),\n",
    "    \"RF\": (RandomForestClassifier, RF_param_grid),\n",
    "    \"GNB\": (GaussianNB, Gauss_param_grid),\n",
    "    \"SVM\": (SVC, SVM_param_grid),\n",
    "    \"LogReg\": (LogisticRegression, Regres_param_grid),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrixes_train = {\"Skel\": dict(), \"Clothes\":dict() , \"Face\":dict() }\n",
    "\n",
    "\n",
    "for f_X, f_y, f_X_2, f_y_2 in samples:\n",
    "\n",
    "    \n",
    "    X = np.load(config.SAMPLED_PATH + f_X)\n",
    "    y = np.load(config.SAMPLED_PATH + f_y)\n",
    "    X_skel, X_clothes, X_face, _, ___, ____, y_train, _____, y_face, ______ = filter_and_split_dataset(X, y)\n",
    "\n",
    "    X_2 = np.load(config.SAMPLED_PATH + f_X_2)\n",
    "    y_2 = np.load(config.SAMPLED_PATH + f_y_2)\n",
    "    X_skel_test, X_clothes_test, X_face_test, _, __, ___, y_test, ____, y_face_test, _____ = filter_and_split_dataset(X_2, y_2)\n",
    "\n",
    "\n",
    "    for name, model in models.items():\n",
    "        printable = False\n",
    "        if name not in confusion_matrixes_train[\"Skel\"]:\n",
    "            confusion_matrixes_train[\"Skel\"][name] = []\n",
    "            confusion_matrixes_train[\"Clothes\"][name] = []\n",
    "            confusion_matrixes_train[\"Face\"][name] = []\n",
    "        if printable:\n",
    "            print(name)\n",
    "        current_model = model\n",
    "        best_param_table = PrettyTable()\n",
    "        best_param_table.title = model\n",
    "        best_param_table.field_names = [\" \",\"scoring\"] + list(current_model[1].keys()) + [\"score\"]\n",
    "        if printable:\n",
    "            print(\"Tuning...\")\n",
    "        best_accuracy_params_skel = tune_model(\"Skeleton\",current_model[0](), current_model[1], 'accuracy', X_skel, y_train, -1)\n",
    "        best_f1macro_params_skel = tune_model(\"Skeleton\",current_model[0](), current_model[1], 'f1_macro',  X_skel, y_train, -1)\n",
    "        best_accuracy_params_clothes = tune_model(\"Clothes\",current_model[0](), current_model[1], 'accuracy', X_clothes, y_train, -1)\n",
    "        best_f1macro_params_clothes = tune_model(\"Clothes\",current_model[0](), current_model[1], 'f1_macro',  X_clothes, y_train, -1)\n",
    "        best_accuracy_params_face = tune_model(\"Face\",current_model[0](), current_model[1], 'accuracy', X_face, y_face, -1)\n",
    "        best_f1macro_params_face = tune_model(\"Face\",current_model[0](), current_model[1], 'f1_macro',  X_face, y_face, -1)\n",
    "        if f_X == \"X_1.npy\" and f_y == \"y_1.npy\":\n",
    "            printable = True\n",
    "            print(best_param_table)\n",
    "        tuned_models_skel = {\n",
    "            'Skeleton accuracy': current_model[0](**best_accuracy_params_skel),\n",
    "            'Skeleton f1_macro': current_model[0](**best_f1macro_params_skel)\n",
    "        }\n",
    "\n",
    "        tuned_models_clothes = {\n",
    "            'Clothes accuracy': current_model[0](**best_accuracy_params_clothes),\n",
    "            'Clothes f1_macro': current_model[0](**best_f1macro_params_clothes)\n",
    "        }\n",
    "\n",
    "        tuned_models_face = {\n",
    "            'Face accuracy': current_model[0](**best_accuracy_params_face),\n",
    "            'Face f1_macro': current_model[0](**best_f1macro_params_face)\n",
    "        }\n",
    "        fit_models(tuned_models_skel, X_skel, y_train)\n",
    "        fit_models(tuned_models_clothes, X_clothes, y_train)\n",
    "        fit_models(tuned_models_face, X_face, y_face)\n",
    "        if printable:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Skeleton\")\n",
    "        confusion_matrixes_train[\"Skel\"][name].append(evaluate_models(tuned_models_skel,model, X_skel, y_train,printable))\n",
    "        if printable:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Clothes\")\n",
    "            print(\"-\"*50)\n",
    "        confusion_matrixes_train[\"Clothes\"][name].append(evaluate_models(tuned_models_clothes,model, X_clothes, y_train,printable))\n",
    "        if printable:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Face\")\n",
    "            print(\"-\"*50)\n",
    "        confusion_matrixes_train[\"Face\"][name].append(evaluate_models(tuned_models_face,model, X_face, y_face,printable))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['{}'.format(col) for col in [\"Skeleton\", \"Clothes\", \"Face\"]]\n",
    "rows = ['{}'.format(row) for row in models.keys()]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(models), ncols=3, figsize=(15,15))\n",
    "for i in range(len(models)):\n",
    "    name = list(models.keys())[i]\n",
    "\n",
    "    d1 = ConfusionMatrixDisplay(np.mean(confusion_matrixes_train[\"Skel\"][name],axis=0),display_labels= class_names)\n",
    "    d1.plot(ax=axes[i,0], cmap=plt.cm.Purples)\n",
    "    d2 = ConfusionMatrixDisplay(np.mean(confusion_matrixes_train[\"Clothes\"][name],axis=0),display_labels= class_names)\n",
    "    d2.plot(ax=axes[i,1], cmap=plt.cm.Purples)\n",
    "    d3 = ConfusionMatrixDisplay(np.mean(confusion_matrixes_train[\"Face\"][name],axis=0),display_labels= class_names)\n",
    "    d3.plot(ax=axes[i,2], cmap=plt.cm.Purples)\n",
    "    \n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=0, size='large')\n",
    "\n",
    "fig.suptitle('Confusion Matrixes Train', fontsize=16)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n",
    "fig.savefig('confusion_matrixes_train.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrixes_test = {\"Skel\": dict(), \"Clothes\":dict() , \"Face\":dict() }\n",
    "\n",
    "\n",
    "for f_X, f_y, f_X_2, f_y_2 in samples:\n",
    "\n",
    "    printable = False\n",
    "    X = np.load(config.SAMPLED_PATH + f_X)\n",
    "    y = np.load(config.SAMPLED_PATH + f_y)\n",
    "    X_skel, X_clothes, X_face, _, ___, ____, y_train, _____, y_face, ______ = filter_and_split_dataset(X, y)\n",
    "\n",
    "    X_2 = np.load(config.SAMPLED_PATH + f_X_2)\n",
    "    y_2 = np.load(config.SAMPLED_PATH + f_y_2)\n",
    "    X_skel_test, X_clothes_test, X_face_test, _, __, ___, y_test, ____, y_face_test, _____ = filter_and_split_dataset(X_2, y_2)\n",
    "\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name not in confusion_matrixes_test[\"Skel\"]:\n",
    "            confusion_matrixes_test[\"Skel\"][name] = []\n",
    "            confusion_matrixes_test[\"Clothes\"][name] = []\n",
    "            confusion_matrixes_test[\"Face\"][name] = []\n",
    "        print(name)\n",
    "        current_model = model\n",
    "        best_param_table = PrettyTable()\n",
    "        best_param_table.title = model\n",
    "        best_param_table.field_names = [\" \",\"scoring\"] + list(current_model[1].keys()) + [\"score\"]\n",
    "        print(\"Tuning...\")\n",
    "        best_accuracy_params_skel = tune_model(\"Skeleton\",current_model[0](), current_model[1], 'accuracy', X_skel, y_train, -1)\n",
    "        best_f1macro_params_skel = tune_model(\"Skeleton\",current_model[0](), current_model[1], 'f1_macro',  X_skel, y_train, -1)\n",
    "        best_accuracy_params_clothes = tune_model(\"Clothes\",current_model[0](), current_model[1], 'accuracy', X_clothes, y_train, -1)\n",
    "        best_f1macro_params_clothes = tune_model(\"Clothes\",current_model[0](), current_model[1], 'f1_macro',  X_clothes, y_train, -1)\n",
    "        best_accuracy_params_face = tune_model(\"Face\",current_model[0](), current_model[1], 'accuracy', X_face, y_face, -1)\n",
    "        best_f1macro_params_face = tune_model(\"Face\",current_model[0](), current_model[1], 'f1_macro',  X_face, y_face, -1)\n",
    "        if f_X == \"X_1.npy\" and f_y == \"y_1.npy\":\n",
    "            print(best_param_table)\n",
    "            printable = True\n",
    "        tuned_models_skel = {\n",
    "            'Skeleton accuracy': current_model[0](**best_accuracy_params_skel),\n",
    "            'Skeleton f1_macro': current_model[0](**best_f1macro_params_skel)\n",
    "        }\n",
    "\n",
    "        tuned_models_clothes = {\n",
    "            'Clothes accuracy': current_model[0](**best_accuracy_params_clothes),\n",
    "            'Clothes f1_macro': current_model[0](**best_f1macro_params_clothes)\n",
    "        }\n",
    "\n",
    "        tuned_models_face = {\n",
    "            'Face accuracy': current_model[0](**best_accuracy_params_face),\n",
    "            'Face f1_macro': current_model[0](**best_f1macro_params_face)\n",
    "        }\n",
    "        fit_models(tuned_models_skel, X_skel, y_train)\n",
    "        fit_models(tuned_models_clothes, X_clothes, y_train)\n",
    "        fit_models(tuned_models_face, X_face, y_face)\n",
    "        if printable:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Skeleton\")\n",
    "        confusion_matrixes_test[\"Skel\"][name].append(evaluate_models(tuned_models_skel,model, X_skel_test, y_test,printable))\n",
    "        if printable:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Clothes\")\n",
    "            print(\"-\"*50)\n",
    "        confusion_matrixes_test[\"Clothes\"][name].append(evaluate_models(tuned_models_clothes,model, X_clothes_test, y_test,printable))\n",
    "        if printable:\n",
    "            print(\"-\"*50)\n",
    "            print(\"Face\")\n",
    "            print(\"-\"*50)\n",
    "        confusion_matrixes_test[\"Face\"][name].append(evaluate_models(tuned_models_face,model, X_face_test, y_face_test,printable))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['{}'.format(col) for col in [\"Skeleton\", \"Clothes\", \"Face\"]]\n",
    "rows = ['{}'.format(row) for row in models.keys()]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(models), ncols=3, figsize=(15,15))\n",
    "for i in range(len(models)):\n",
    "    name = list(models.keys())[i]\n",
    "\n",
    "    d1 = ConfusionMatrixDisplay(np.mean(confusion_matrixes_test[\"Skel\"][name],axis=0),display_labels= class_names)\n",
    "    d1.plot(ax=axes[i,0], cmap=plt.cm.Purples)\n",
    "    d2 = ConfusionMatrixDisplay(np.mean(confusion_matrixes_test[\"Clothes\"][name],axis=0),display_labels= class_names)\n",
    "    d2.plot(ax=axes[i,1], cmap=plt.cm.Purples)\n",
    "    d3 = ConfusionMatrixDisplay(np.mean(confusion_matrixes_test[\"Face\"][name],axis=0),display_labels= class_names)\n",
    "    d3.plot(ax=axes[i,2], cmap=plt.cm.Purples)\n",
    "    \n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=0, size='large')\n",
    "\n",
    "fig.suptitle('Confusion Matrixes Test', fontsize=16)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n",
    "fig.savefig('confusion_matrixes_test.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "The following block contains the final model and the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, policy='argmax', name = \"FinalModel\"):\n",
    "        self.model_skel = RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200, random_state = 42)\n",
    "        self.model_clothes = RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 10, n_estimators = 100, random_state = 4)\n",
    "        self.model_face = SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42)\n",
    "        self.name = name\n",
    "        self.classes_ = [0, 1]\n",
    "        self.policy = policy\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # prepare data\n",
    "        features_skel = X[:, 0:9]\n",
    "        features_clothes =  X[:, 9:9 + config.NUM_POINTS_LBP]\n",
    "        features_face_not_filtered = X[:, 9 + config.NUM_POINTS_LBP:]\n",
    "        features_face, y_face = filter_faces(features_face_not_filtered, y)\n",
    "\n",
    "        # fit models\n",
    "        self.model_skel.fit(features_skel, y)\n",
    "        self.model_clothes.fit(features_clothes, y)\n",
    "        self.model_face.fit(features_face, y_face)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.policy == 'argmax':\n",
    "            return np.argmax(self.predict_proba(X), axis=1)\n",
    "        elif self.policy == 'or':\n",
    "            features_skel = X[:, 0:9]\n",
    "            features_clothes =  X[:, 9:9 + config.NUM_POINTS_LBP]\n",
    "            features_face_not_filtered = X[:, 9 + config.NUM_POINTS_LBP:]\n",
    "            \n",
    "            indexes = np.argwhere(~np.isnan(features_face_not_filtered[:, 0].reshape(-1)))\n",
    "            features_face = features_face_not_filtered[indexes.flatten()]\n",
    "            \n",
    "            output_skel = self.model_skel.predict(features_skel)\n",
    "            output_clothes = self.model_clothes.predict(features_clothes)\n",
    "            o_face = self.model_face.predict(features_face)\n",
    "\n",
    "            output_face = np.zeros(len(X))\n",
    "            np.put(output_face, indexes, o_face)\n",
    "\n",
    "            return np.bitwise_or(output_skel, output_clothes, output_face)\n",
    "        else:\n",
    "            raise Exception(f\"Policy not supported: {self.policy}. Please use \\'or\\' | \\'argmax\\'\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        features_skel = X[:, 0:9]\n",
    "        features_clothes =  X[:, 9:9 + config.NUM_POINTS_LBP]\n",
    "        features_face_not_filtered = X[:, 9 + config.NUM_POINTS_LBP:]\n",
    "\n",
    "        # Get valid faces\n",
    "        indexes = np.argwhere(~np.isnan(features_face_not_filtered[:, 0].reshape(-1)))\n",
    "        features_face = features_face_not_filtered[indexes.flatten()]\n",
    "        \n",
    "        # Predict\n",
    "        output_skel = self.model_skel.predict_proba(features_skel)\n",
    "        output_clothes = self.model_clothes.predict_proba(features_clothes)\n",
    "        o_face = self.model_face.predict_proba(features_face)\n",
    "\n",
    "        # Insert face outputs in a new array\n",
    "        output_face = np.asarray([np.asarray([0, 0]) for _ in range(len(X))])\n",
    "        np.put(output_face, indexes, o_face)\n",
    "\n",
    "        # calculate final probabilities\n",
    "        output = (output_skel + output_clothes + output_face) / 3\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = [f for f in os.listdir(\"./sampled\") if re.match(\"(video_)([0-9]*)(_sample_)([0-9]*)_([Xy])(.npy)\", f)]\n",
    "files.sort()\n",
    "samples = [(files[i], files[i+1], files[i+2], files[i+3]) for i in range(0, len(files) - 2, 4)]\n",
    "\n",
    "fig, (ax_roc, ax_det) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(14, 6)\n",
    "\n",
    "FINAL_MODEL_POLICY = \"argmax\"\n",
    "\n",
    "fprs = []\n",
    "tprs = []\n",
    "fprs2 = []\n",
    "tprs2 = []\n",
    "roc_aucs = []\n",
    "cms = []\n",
    "# fit the model for each training sample and average the fpr, tpr and roc_auc\n",
    "for f_X, f_y, f_X_2, f_y_2 in samples:\n",
    "    print([f_X, f_y, f_X_2, f_y_2])\n",
    "\n",
    "    X = np.load(\"./sampled/\" + f_X)\n",
    "    y = np.load(\"./sampled/\" + f_y)\n",
    "\n",
    "    X_2 = np.load(\"./sampled/\" + f_X_2)\n",
    "    y_2 = np.load(\"./sampled/\" + f_y_2)\n",
    "\n",
    "    model = FinalModel(policy = FINAL_MODEL_POLICY)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    res = model.predict_proba(X_2)[:, 1]\n",
    "\n",
    "    pr = model.predict(X_2)\n",
    "\n",
    "    print(classification_report(y_2, pr))\n",
    "\n",
    "    cms.append(confusion_matrix(y_2, pr))\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_2, res)\n",
    "    auc = roc_auc_score(y_2, res)\n",
    "    roc_aucs.append(auc)\n",
    "\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "    fpr2, tpr2, thresholds2 = det_curve(y_2, res)\n",
    "    fprs2.append(fpr2)\n",
    "    tprs2.append(tpr2)\n",
    "\n",
    "longest_fpr = len(max(fprs, key=len))\n",
    "longest_tpr = len(max(tprs, key=len))\n",
    "longest_fpr2 = len(max(fprs2, key=len))\n",
    "longest_tpr2 = len(max(tprs2, key=len))\n",
    "\n",
    "# pad the shorter arrays with the last value\n",
    "for i in range(0, len(fprs)):\n",
    "    fprs[i] = np.pad(fprs[i], (0, longest_fpr - len(fprs[i])), 'edge')\n",
    "    tprs[i] = np.pad(tprs[i], (0, longest_tpr - len(tprs[i])), 'edge')\n",
    "\n",
    "for i in range(0, len(fprs2)):\n",
    "    fprs2[i] = np.pad(fprs2[i], (0, longest_fpr2 - len(fprs2[i])), 'edge')\n",
    "    tprs2[i] = np.pad(tprs2[i], (0, longest_tpr2 - len(tprs2[i])), 'edge')\n",
    "\n",
    "# average the fpr and tpr and calculate the auc\n",
    "fpr = np.mean(fprs, axis=0)\n",
    "tpr = np.mean(tprs, axis=0)\n",
    "fpr2 = np.mean(fprs2, axis=0)\n",
    "tpr2 = np.mean(tprs2, axis=0)\n",
    "\n",
    "auc = np.mean(roc_aucs)\n",
    "\n",
    "cms = np.asarray(cms)\n",
    "print(cms.shape)\n",
    "cm = np.mean(cms, axis=0)\n",
    "\n",
    "fig_cm, ax_cm = plt.subplots(1, 1)\n",
    "ax_cm.set_title(f\"FinalModel - {FINAL_MODEL_POLICY}\")\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "ax_cm = disp.plot(cmap=plt.cm.Purples, ax=ax_cm)\n",
    "plt.savefig(f\"final_model_{FINAL_MODEL_POLICY}_cm.png\")\n",
    "\n",
    "ax_roc.set_title(\"FinalModel - ROC\")\n",
    "ax_det.set_title(\"FinalModel - DET\")\n",
    "\n",
    "\n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_det.set_xlabel(\"Detection Error Rate\")\n",
    "\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "ax_det.set_ylabel(\"Detection Rate\")\n",
    "\n",
    "# Add Legends\n",
    "\n",
    "ax_roc.grid(linestyle=\"--\")\n",
    "ax_det.grid(linestyle=\"--\")\n",
    "\n",
    "\n",
    "ax_roc.plot(fpr, tpr, label=\"FinalModel (AUC = \" + str(round(auc, 2)) + \")\")\n",
    "ax_det.plot(fpr2, tpr2, label=\"FinalModel\")\n",
    "ax_roc.legend(loc=\"lower right\")\n",
    "ax_det.legend(loc=\"upper right\")\n",
    "fig.savefig(f\"final_model_roc_det.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We also thought that it was possible to learn a \"latent space\" that ensambles that person's features. The idea is that a regressor can learn how to optimize the outpot of a linear combination so that features from the person that we want to re-identify will output a low value (0) while the other people will output an higher value (1).\n",
    "\n",
    "Given that, we used the AdaBoost with SVR regressors in a pipeline in order to learn the features of a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "    \n",
    "def test_model_(name, model, X_train, X_test, y_train, y_test, ax):\n",
    "    # fit the model on the training set\n",
    "    model.fit(X_train, y_train)\n",
    "    # Test\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    rerank = [np.asarray([a, b]) for a, b in zip(pred, y_test)]\n",
    "    rerank.sort(key=lambda x: x[0])\n",
    "\n",
    "    other = target = 0\n",
    "\n",
    "    l = []\n",
    "    for idx, element in enumerate(rerank):\n",
    "        if element[1] == 0:\n",
    "            target += 1\n",
    "        else:\n",
    "            other += 1\n",
    "        l.append(target)\n",
    "                \n",
    "    # Plots\n",
    "\n",
    "    if ax is not None: \n",
    "        print(\"-\" * 10)\n",
    "        print(\"|\", name, \"|\")\n",
    "        print(\"-\" * 10)    \n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(X_train)\n",
    "        print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.scatter(\n",
    "            pca_result[:, 0], pca_result[:, 1],\n",
    "            c=y_train,\n",
    "            cmap='prism'\n",
    "        )\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "        plt.show()\n",
    "\n",
    "        # tsne = TSNE(n_components=2, verbose=1, perplexity=100, n_iter=1000)\n",
    "        # tsne_results = tsne.fit_transform(X_train, y=y_train)\n",
    "\n",
    "        # plt.figure(figsize=(10,6))\n",
    "        # plt.scatter(\n",
    "        #     tsne_results[:, 0], tsne_results[:, 1],\n",
    "        #     c=y_train,\n",
    "        #     cmap='prism'\n",
    "        # )\n",
    "        # plt.xlabel('t-SNE Component 1')\n",
    "        # plt.ylabel('t-SNE Component 2')\n",
    "        # plt.show()\n",
    "    \n",
    "        print(mean_absolute_error(y_test, pred))\n",
    "        print(mean_squared_error(y_test, pred))\n",
    "        plt.plot(y_test, pred, 'o')    \n",
    "        plt.show()\n",
    "    l = np.asarray(l)\n",
    "    return l / (target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the number of true positive (normalized) as long as we consider samples of the re ranked input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "files = [f for f in os.listdir(\"./sampled\") if re.match(\"(video_)([0-9]*)(_sample_)([0-9]*)_([Xy])(.npy)\", f)]\n",
    "files.sort()\n",
    "samples = [(files[i], files[i+1], files[i+2], files[i+3]) for i in range(0, len(files) - 2, 4)]\n",
    "\n",
    "curve_sk = []\n",
    "curve_cl = []\n",
    "curve_fa = []\n",
    "\n",
    "for f_X, f_y, f_X_2, f_y_2 in samples:\n",
    "    print([f_X, f_y, f_X_2, f_y_2])\n",
    "\n",
    "    X = np.load(\"./sampled/\" + f_X)\n",
    "    y = np.load(\"./sampled/\" + f_y)\n",
    "    y -= 1\n",
    "    y *= -1\n",
    "    X_skel, X_clothes, X_face, trhs_skel, trhs_clothes, trhs_faces, y_train, y_trhs, y_face, y_trhs_face = filter_and_split_dataset(X, y)\n",
    "\n",
    "    X_2 = np.load(\"./sampled/\" + f_X_2)\n",
    "    y_2 = np.load(\"./sampled/\" + f_y_2)\n",
    "    y_2 -= 1\n",
    "    y_2 *= -1\n",
    "    X_skel_test, X_clothes_test, X_face_test, _, __, ___, y_test, ____, y_face_test, _____ = filter_and_split_dataset(X_2, y_2)\n",
    "\n",
    "\n",
    "    m1 = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"SVRLogistic\", AdaBoostRegressor(base_estimator=SVR(), n_estimators=30))])\n",
    "    m2 = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"SVRLogistic\", AdaBoostRegressor(base_estimator=SVR(), n_estimators=30))])\n",
    "    m3 = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"SVRLogistic\", SVR())])\n",
    "    # fig, ax = plt.subplots(1, 3)\n",
    "    ax = [None] * 3\n",
    "    curve_sk.append(test_model_(\"Skel\", m1, X_skel, X_skel_test, y_train, y_test, ax[0]))\n",
    "    curve_cl.append(test_model_(\"Clothes\", m2, X_clothes, X_clothes_test, y_train, y_test, ax[1]))\n",
    "    curve_fa.append(test_model_(\"Face\", m3, X_face, X_face_test, y_face, y_face_test, ax[2]))\n",
    "\n",
    "curve_sk = np.mean(curve_sk, axis=0)\n",
    "curve_cl = np.mean(curve_cl, axis=0)\n",
    "x_range = np.arange(0, len(curve_sk), dtype=np.float32) / len(curve_sk)\n",
    "plt.plot(x_range, curve_sk, label=\"skeleton\")\n",
    "plt.plot(x_range, curve_cl, label=\"clothes\")\n",
    "\n",
    "# padding\n",
    "\n",
    "longest_fa = len(max(curve_fa, key=len))\n",
    "\n",
    "# pad the shorter arrays with the last value\n",
    "for i in range(0, len(curve_fa)):\n",
    "    curve_fa[i] = np.pad(curve_fa[i], (0, longest_fa - len(curve_fa[i])), 'edge')\n",
    "\n",
    "curve_fa = np.mean(curve_fa, axis=0)\n",
    "x_range_fa = np.arange(0, len(curve_fa), dtype=np.float32)\n",
    "plt.plot(x_range_fa / len(curve_fa), curve_fa, label=\"face\")\n",
    "plt.title(\"Regression - ranking\")\n",
    "plt.xlabel(\"Number of samples (normalized)\")\n",
    "plt.ylabel(\"number of true labels (normalized)\")\n",
    "\n",
    "# Add Legends\n",
    "\n",
    "plt.grid(linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FinalRegressor\n",
    "\n",
    "final regressor made by the sum of three minor regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Called when initializing the classifier\n",
    "        \"\"\"\n",
    "        self.model_skel = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"SVRLogistic\", AdaBoostRegressor(base_estimator=SVR(), n_estimators=30))])\n",
    "        self.model_clothes = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"SVRLogistic\", AdaBoostRegressor(base_estimator=SVR(), n_estimators=30))])\n",
    "        self.model_face = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"SVRLogistic\", SVR())])\n",
    "\n",
    "    def fit(self, X, y=None):        \n",
    "        features_skel = X[:, 0:9]\n",
    "        features_clothes =  X[:, 9:9 + config.NUM_POINTS_LBP]\n",
    "        features_face_not_filtered = X[:, 9 + config.NUM_POINTS_LBP:]\n",
    "        features_face, y_face = filter_faces(features_face_not_filtered, y)\n",
    "\n",
    "        # fit models\n",
    "        self.model_skel.fit(features_skel, y)\n",
    "        self.model_clothes.fit(features_clothes, y)\n",
    "        self.model_face.fit(features_face, y_face)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        features_skel = X[:, 0:9]\n",
    "        features_clothes =  X[:, 9:9 + config.NUM_POINTS_LBP]\n",
    "        features_face_not_filtered = X[:, 9 + config.NUM_POINTS_LBP:]\n",
    "            \n",
    "        indexes = np.argwhere(~np.isnan(features_face_not_filtered[:, 0].reshape(-1)))\n",
    "        features_face = features_face_not_filtered[indexes.flatten()]\n",
    "        \n",
    "        output_skel = self.model_skel.predict(features_skel)\n",
    "        output_clothes = self.model_clothes.predict(features_clothes)\n",
    "        o_face = self.model_face.predict(features_face)\n",
    "\n",
    "        output_face = np.ones(len(X))\n",
    "        np.put(output_face, indexes, o_face)\n",
    "\n",
    "        return (output_skel + output_clothes + output_face) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "files = [f for f in os.listdir(\"./sampled\") if re.match(\"(video_)([0-9]*)(_sample_)([0-9]*)_([Xy])(.npy)\", f)]\n",
    "files.sort()\n",
    "samples = [(files[i], files[i+1], files[i+2], files[i+3]) for i in range(0, len(files) - 2, 4)]\n",
    "\n",
    "curve_fr = []\n",
    "\n",
    "for f_X, f_y, f_X_2, f_y_2 in samples:\n",
    "    print([f_X, f_y, f_X_2, f_y_2])\n",
    "\n",
    "    X = np.load(\"./sampled/\" + f_X)\n",
    "    y = np.load(\"./sampled/\" + f_y)\n",
    "    y -= 1\n",
    "    y *= -1\n",
    "\n",
    "    X_2 = np.load(\"./sampled/\" + f_X_2)\n",
    "    y_2 = np.load(\"./sampled/\" + f_y_2)\n",
    "    y_2 -= 1\n",
    "    y_2 *= -1\n",
    "\n",
    "    m = FinalRegressor()\n",
    "    # fig, ax = plt.subplots(1, 3)\n",
    "    ax = [None] * 3\n",
    "    curve_fr.append(test_model_(\"Skel\", m, X, X_2, y, y_2, ax[0]))\n",
    "\n",
    "\n",
    "curve_fr = np.mean(curve_fr, axis=0)\n",
    "x_range = np.arange(0, len(curve_fr), dtype=np.float32) / len(curve_fr)\n",
    "plt.plot(x_range, curve_fr, label=\"FinalRegressor\")\n",
    "\n",
    "# padding\n",
    "\n",
    "plt.title(\"FinalRegressor - ranking\")\n",
    "plt.xlabel(\"Number of samples (normalized)\")\n",
    "plt.ylabel(\"number of true labels (normalized)\")\n",
    "\n",
    "# Add Legends\n",
    "\n",
    "plt.grid(linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92b87d66e1b895e7245a4bdd6e25962cd95b6ead1bc6c6c776b8b175255c756d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
