{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: ReID\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We wanted to reidentify a person, given some samples of the dataset. The idea is to learn the main features of the person in order to distinguish him/her from other people.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we did was to import libraries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO\n",
    "- Write just one model\n",
    "- Better frames for datasets\n",
    "- Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "from load_data import DataLoader\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, Normalizer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import config\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from utils import filter_and_split_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataLoader()\n",
    "d.read_dataset()\n",
    "d.shuffle_videos()\n",
    "d.display_dataset()\n",
    "\n",
    "metrics = []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do was to design the learning setting. The first and easiest idea was to test a variety of classifiers, and see which works better for our features.\n",
    "\n",
    "We tested different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(name, model_skel, model_clothes, model_face, X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test):\n",
    "    global metrics\n",
    "    model_skel.fit(X_skel, y_train)\n",
    "    model_clothes.fit(X_clothes, y_train)\n",
    "    model_face.fit(X_face, y_face)\n",
    "\n",
    "\n",
    "    print(\"-\" * 10)\n",
    "    print(\"|\", name, \"|\")\n",
    "    # Test\n",
    "    print(\"-\" * 10)\n",
    "    skel_pred = model_skel.predict(X_skel_test)\n",
    "    print(classification_report(y_test, skel_pred))\n",
    "    print(\"-\" * 10)\n",
    "    clothes_pred = model_clothes.predict(X_clothes_test)\n",
    "    print(classification_report(y_test, clothes_pred))\n",
    "    print(\"-\" * 10)\n",
    "    face_pred = model_face.predict(X_face_test)\n",
    "    print(classification_report(y_face_test, face_pred))\n",
    "\n",
    "    skel_pred_roc = model_skel.predict_proba(X_skel_test)\n",
    "    clothes_pred_roc = model_clothes.predict_proba(X_clothes_test)\n",
    "    face_pred_roc = model_face.predict_proba(X_face_test)\n",
    "\n",
    "    metrics.append([name.split(\"_\")[-1].split(\".\")[-1], y_test, y_face_test, skel_pred, clothes_pred, face_pred, skel_pred_roc, clothes_pred_roc, face_pred_roc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(config.SAMPLED_PATH + \"video_0_sample_0_X.npy\")\n",
    "y = np.load(config.SAMPLED_PATH + \"video_0_sample_0_y.npy\")\n",
    "X_skel, X_clothes, X_face, _, ___, ____, y_train, _____, y_face, ______ = filter_and_split_dataset(X, y)\n",
    "\n",
    "X_2 = np.load(config.SAMPLED_PATH + \"video_0_sample_1_X.npy\")\n",
    "y_2 = np.load(config.SAMPLED_PATH + \"video_0_sample_1_y.npy\")\n",
    "X_skel_test, X_clothes_test, X_face_test, _, __, ___, y_test, ____, y_face_test, _____ = filter_and_split_dataset(X_2, y_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import DetCurveDisplay, RocCurveDisplay\n",
    "\n",
    "classifiers_skel = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=2, min_samples_split=10, random_state=2, splitter='random'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200, random_state = 42),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=3, p=1, weights='distance'),\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=10000),\n",
    "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
    "    \"SVC\": SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42),\n",
    "}\n",
    "\n",
    "classifiers_clothes = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=1, min_samples_split=10, random_state=2, splitter='best'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 10, n_estimators = 100, random_state = 4),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=7, p=1, weights='distance'),\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=10000),\n",
    "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
    "    \"SVC\": SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42),\n",
    "}\n",
    "\n",
    "classifiers_face = {\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(criterion='gini', max_depth=None, max_features='sqrt', min_samples_leaf=2, min_samples_split=5, random_state=2, splitter='random'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 50, random_state = 42),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=9, p=1, weights='distance'),\n",
    "    \"LogisticRegression\": LogisticRegression(solver=\"liblinear\", max_iter=10000),\n",
    "    \"GaussianNB\": GaussianNB(var_smoothing=1e-09),\n",
    "    \"SVC\": SVC(C = 1, kernel = \"linear\", degree = 3, gamma = \"auto\", probability = True, tol = 0.001, random_state = 42),\n",
    "}\n",
    "\n",
    "fig, ((ax_roc_skel, ax_roc_clothes, ax_roc_face), (ax_det_skel, ax_det_clothes, ax_det_face)) = plt.subplots(2, 3, figsize =(20, 10))\n",
    "# fig, (ax_roc_skel, ax_roc_clothes, ax_roc_face, ax_det_skel, ax_det_clothes, ax_det_face) = plt.subplots(1, 6, figsize=(50,5))\n",
    "\n",
    "for name, model in classifiers_skel.items():\n",
    "    model.fit(X_skel, y_train)\n",
    "    RocCurveDisplay.from_estimator(model, X_skel_test, y_test, ax=ax_roc_skel, name=name)\n",
    "    DetCurveDisplay.from_estimator(model, X_skel_test, y_test, ax=ax_det_skel, name=name)\n",
    "\n",
    "for name, model in classifiers_clothes.items():\n",
    "    model.fit(X_clothes, y_train)\n",
    "    RocCurveDisplay.from_estimator(model, X_clothes_test, y_test, ax=ax_roc_clothes, name=name)\n",
    "    DetCurveDisplay.from_estimator(model, X_clothes_test, y_test, ax=ax_det_clothes, name=name)\n",
    "\n",
    "for name, model in classifiers_face.items():\n",
    "    model.fit(X_face, y_face)\n",
    "    RocCurveDisplay.from_estimator(model, X_face_test, y_face_test, ax=ax_roc_face, name=name)\n",
    "    DetCurveDisplay.from_estimator(model, X_face_test, y_face_test, ax=ax_det_face, name=name)\n",
    "\n",
    "\n",
    "ax_roc_skel.set_title(\"Receiver Operating Characteristic (ROC) curves - Skeleton\")\n",
    "ax_det_skel.set_title(\"Detection Error Tradeoff (DET) curves - Skeleton\")\n",
    "ax_roc_clothes.set_title(\"Receiver Operating Characteristic (ROC) curves - Clothes\")\n",
    "ax_det_clothes.set_title(\"Detection Error Tradeoff (DET) curves - Clothes\")\n",
    "ax_roc_face.set_title(\"Receiver Operating Characteristic (ROC) curves - Face\")\n",
    "ax_det_face.set_title(\"Detection Error Tradeoff (DET) curves - Face\")\n",
    "\n",
    "\n",
    "ax_roc_skel.grid(linestyle=\"--\")\n",
    "ax_det_skel.grid(linestyle=\"--\")\n",
    "ax_roc_clothes.grid(linestyle=\"--\")\n",
    "ax_det_clothes.grid(linestyle=\"--\")\n",
    "ax_roc_face.grid(linestyle=\"--\")\n",
    "ax_det_face.grid(linestyle=\"--\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"ROC_DET_curves.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "class_names = ['Different', 'Same']\n",
    "\n",
    "def fit_models(models, X_train, y_train):\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train.ravel())\n",
    "        print(name, 'trained.')\n",
    "    return\n",
    "\n",
    "def evaluate_models(models,model_name,X_test, y_test):\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred,zero_division=0))\n",
    "        cv_accuracy = cross_val_score(model, X_test, y_test.ravel(), n_jobs=-1, scoring='accuracy')\n",
    "        cv_f1_macro = cross_val_score(model, X_test, y_test.ravel(), n_jobs=-1, scoring='f1_macro')\n",
    "        print(cross_val_score(model, X_test, y_test, scoring='accuracy'))\n",
    "        print(\"Accuracy: %0.4f (+/- %0.4f)\" % (cv_accuracy.mean(), cv_accuracy.std() * 2))\n",
    "        print(cross_val_score(model, X_test, y_test, scoring='f1_macro'))\n",
    "        print(\"f1-score: %0.4f (+/- %0.4f)\" % (cv_f1_macro.mean(), cv_f1_macro.std() * 2))\n",
    "        if name.count(\"accuracy\")> 0:\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "            disp.plot(cmap=plt.cm.Blues)\n",
    "            disp.ax_.set_title(name + ' (' + model_name + ')') \n",
    "        print('------------------------------------'*3)\n",
    "    return  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_model(model, param_grid, scoring, x_train, y_train, grid_jobs):\n",
    "    print('tuning...')\n",
    "    clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, verbose=1, n_jobs=grid_jobs)\n",
    "    clf.fit(x_train, y_train.ravel())\n",
    "    print('done')\n",
    "    print()\n",
    "    print(\"Best: %f using %s\" % (clf.best_score_, clf.best_params_))\n",
    "    best_params = clf.best_params_.copy()\n",
    "    return best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SVM_param_grid = {\n",
    "    'C': [0.5, 1],\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid', 'linear'],\n",
    "    'degree': [1,2,3,4,5],\n",
    "    'gamma': ['scale'],\n",
    "    'random_state': [2,42]\n",
    "}\n",
    "\n",
    "RF_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'random_state': [2,42]\n",
    "}\n",
    "\n",
    "\n",
    "Gauss_param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "KNN_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 20, 30, 40],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "DT_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'random_state': [2,42]\n",
    "}\n",
    "\n",
    "Regres_param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'dual': [True, False],\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "    'C': [0.5, 1],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [1, 2, 3],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'random_state': [2,42],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'multi_class': ['auto', 'ovr', 'multinomial'],\n",
    "    'verbose': [0],\n",
    "    'warm_start': [True, False],\n",
    "    'n_jobs': [-1],\n",
    "    'l1_ratio': [None]\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"KNN\": (KNeighborsClassifier, KNN_param_grid),\n",
    "    \"DT\": (DecisionTreeClassifier, DT_param_grid),\n",
    "    \"RF\": (RandomForestClassifier, RF_param_grid),\n",
    "    \"GNB\": (GaussianNB, Gauss_param_grid),\n",
    "    \"SVM\": (SVC, SVM_param_grid),\n",
    "    # \"LogReg\": (LogisticRegression(), Regres_param_grid),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    current_model = models[model]\n",
    "    best_accuracy_params_skel = tune_model(current_model[0](), current_model[1], 'accuracy', X_skel, y_train, -1)\n",
    "    best_f1macro_params_skel = tune_model(current_model[0](), current_model[1], 'f1_macro',  X_skel, y_train, -1)\n",
    "    best_accuracy_params_clothes = tune_model(current_model[0](), current_model[1], 'accuracy', X_clothes, y_train, -1)\n",
    "    best_f1macro_params_clothes = tune_model(current_model[0](), current_model[1], 'f1_macro',  X_clothes, y_train, -1)\n",
    "    best_accuracy_params_face = tune_model(current_model[0](), current_model[1], 'accuracy', X_face, y_face, -1)\n",
    "    best_f1macro_params_face = tune_model(current_model[0](), current_model[1], 'f1_macro',  X_face, y_face, -1)\n",
    "    tuned_models_skel = {\n",
    "        'Skeleton accuracy': current_model[0](**best_accuracy_params_skel),\n",
    "        'Skeleton f1_macro': current_model[0](**best_f1macro_params_skel)\n",
    "    }\n",
    "\n",
    "    tuned_models_clothes = {\n",
    "        'Clothes accuracy': current_model[0](**best_accuracy_params_clothes),\n",
    "        'Clothes f1_macro': current_model[0](**best_f1macro_params_clothes)\n",
    "    }\n",
    "\n",
    "    tuned_models_face = {\n",
    "        'Face accuracy': current_model[0](**best_accuracy_params_face),\n",
    "        'Face f1_macro': current_model[0](**best_f1macro_params_face)\n",
    "    }\n",
    "        \n",
    "    fit_models(tuned_models_skel, X_skel, y_train)\n",
    "    fit_models(tuned_models_clothes, X_clothes, y_train)\n",
    "    fit_models(tuned_models_face, X_face, y_face)\n",
    "    print(\"-\"*50)\n",
    "    print(\"Skeleton\")\n",
    "    evaluate_models(tuned_models_skel,model, X_skel, y_train)\n",
    "    print(\"-\"*50)\n",
    "    print(\"Clothes\")\n",
    "    print(\"-\"*50)\n",
    "    evaluate_models(tuned_models_clothes,model, X_clothes, y_train)\n",
    "    print(\"-\"*50)\n",
    "    print(\"Face\")\n",
    "    print(\"-\"*50)\n",
    "    evaluate_models(tuned_models_face,model, X_face, y_face)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "| Model | Skeleton | Clothes | Face | \n",
    "| :--- | :---: | :---: | :---: | \n",
    "| DT | 0.91 | 0.93  | 0.81 |\n",
    "| RF | 0.93 | 0.94  | 0.85 |\n",
    "| SVC | 0.91 | 0.93  | 0.81 |\n",
    "| GNB | 0.89 | 0.79  | 0.81 |\n",
    "| KNN | 0.94 | 0.94  | 0.84 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the models on the big dataset\n",
    "\n",
    "X = np.load(config.SAMPLED_PATH + \"video_0_sample_0_X.npy\")\n",
    "y = np.load(config.SAMPLED_PATH + \"video_0_sample_0_y.npy\")\n",
    "X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test = filter_and_split_dataset(X, y)\n",
    "\n",
    "model_skel = KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=3, p=1, weights='distance')#  RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 200, random_state = 42)\n",
    "model_clothes = KNeighborsClassifier(algorithm='auto', leaf_size=10, n_neighbors=7, p=1, weights='distance')# RandomForestClassifier(criterion ='entropy', max_depth = None, max_features ='sqrt', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 100, random_state = 42)\n",
    "model_face = SVC(C = 1, kernel = \"poly\", degree = 3, gamma = \"auto\", probability = True, random_state = 2)\n",
    "\n",
    "\n",
    "model_skel.fit(X_skel, y_train)\n",
    "model_clothes.fit(X_clothes, y_train)\n",
    "model_face.fit(X_face, y_face)\n",
    "\n",
    "# Sample dataset live\n",
    "\n",
    "X, y, images = d.sample_dataset(video=0, return_images=True, seed=10)\n",
    "X_train, _, y_train, __, images_train, ___ = train_test_split(X, y, images, stratify=y)\n",
    "print(y_train)\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(\"skel\")\n",
    "skel_pred = model_skel.predict(X_train[:, : 9])\n",
    "print(classification_report(y_train, skel_pred))\n",
    "print(\"-\" * 10)\n",
    "print(\"clothes\")\n",
    "clothes_pred = model_clothes.predict(X_train[:, 9 : 9 + config.NUM_POINTS_LBP])\n",
    "print(classification_report(y_train, clothes_pred))\n",
    "print(\"-\" * 10)\n",
    "\n",
    "\n",
    "# print(\"face\")\n",
    "# X_face = []\n",
    "# y_face = []\n",
    "\n",
    "# for i in range(len(X_train)):\n",
    "#     if not math.isnan(X_train[i][0]):\n",
    "#         X_face.append(X_train[i])\n",
    "#         y_face.append(y_train[i])\n",
    "        \n",
    "# face_pred = model_face.predict(X_face)\n",
    "# print(classification_report(y_face, face_pred))\n",
    "\n",
    "\n",
    "\n",
    "target = np.where(y_train == 1)\n",
    "target_img = images_train[target[0][0]]\n",
    "\n",
    "fig, ax = plt.subplots(len(images_train) // 5, 5)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    t, el = y_train[i], images_train[i]\n",
    "    ax[i // 5, i % 5].imshow(el)\n",
    "plt.show()\n",
    " \n",
    "\n",
    "for idx in range(len(y_train)):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    ax[0].imshow(images_train[idx])\n",
    "    ax[0].set_title(f\"True label: {y_train[idx]}, predict: {bool(skel_pred[idx] or clothes_pred[idx])}\")\n",
    "    ax[1].imshow(target_img)\n",
    "    ax[1].set_title(\"Target\")#\"skel label: \" + str(bool(res[1][0])), \"clothes label: \" + str(bool(res[1][1])), \"face label: \" + str(bool(res[1][2])))\n",
    "    print(skel_pred[idx], clothes_pred[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We also thought that it was possible to learn a \"latent space\" that ensambles that person's features. The idea is that a regressor can learn how to optimize the outpot of a linear combination so that features from the person that we want to re-identify will output a low value (0) while the other people will output an higher value (1).\n",
    "\n",
    "Given that, we tested some regressors in order to learn the features of a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_logistic, y_logistc, _ = d.sample_dataset(video=0, return_images=False)\n",
    "X = np.load(config.SAMPLED_PATH + \"video_0_sample_0_X.npy\")\n",
    "y = np.load(config.SAMPLED_PATH + \"video_0_sample_0_y.npy\")\n",
    "y -= 1\n",
    "y *= -1\n",
    "X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test = filter_and_split_dataset(X, y)\n",
    "\n",
    "X = np.load(config.SAMPLED_PATH + \"video_0_sample_1_X.npy\")\n",
    "y = np.load(config.SAMPLED_PATH + \"video_0_sample_1_y.npy\")\n",
    "y -= 1\n",
    "y *= -1\n",
    "X_skel_logistic, X_clothes_logistic, X_face_logistic, X_skel_test_logistic, X_clothes_test_logistic, X_face_test_logistic, y_train_logistic, y_test_logistic, y_face_logistic, y_face_test_logistic = filter_and_split_dataset(X, y)\n",
    "c_svr = 0\n",
    "\n",
    "def test_regression(name, model_skel, model_clothes, model_face, \n",
    "        X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test,\n",
    "        X_skel_logistic, X_clothes_logistic, X_face_logistic, X_skel_test_logistic, X_clothes_test_logistic, X_face_test_logistic, y_train_logistic, y_test_logistic, y_face_logistic, y_face_test_logistic):\n",
    "    \n",
    "    model_skel.fit(X_skel, y_train)\n",
    "    model_clothes.fit(X_clothes, y_train)\n",
    "    model_face.fit(X_face, y_face)\n",
    "\n",
    "    # Train the logistic regressor\n",
    "    log_reg_skel, log_reg_clothes, log_reg_face = LogisticRegression(max_iter = 10000), LogisticRegression(max_iter = 10000), LogisticRegression(max_iter = 10000)\n",
    "\n",
    "    log_reg_skel.fit(model_skel.predict(X_skel_logistic).reshape(-1, 1), y_train_logistic)\n",
    "    log_reg_clothes.fit(model_clothes.predict(X_clothes_logistic).reshape(-1, 1), y_train_logistic)\n",
    "    log_reg_face.fit(model_face.predict(X_face_logistic).reshape(-1, 1), y_face_logistic)\n",
    "\n",
    "    # Test\n",
    "    print(\"-\" * 10)\n",
    "    print(\"|\", name, \"|\")\n",
    "    print(\"-\" * 10)\n",
    "    print(\"| skel |\")\n",
    "    skel_pred = model_skel.predict(X_skel_test)\n",
    "\n",
    "    log_skel_pred = log_reg_skel.predict(skel_pred.reshape(-1, 1))\n",
    "\n",
    "    print(classification_report(y_test, log_skel_pred, zero_division=0))\n",
    "    print(mean_absolute_error(y_test, skel_pred))\n",
    "    print(mean_squared_error(y_test, skel_pred))\n",
    "\n",
    "    print(\"| clothes |\")\n",
    "    clotehs_pred = model_clothes.predict(X_clothes_test)\n",
    "    \n",
    "    log_clotehs_pred = log_reg_clothes.predict(clotehs_pred.reshape(-1, 1))\n",
    "\n",
    "    print(classification_report(y_test, log_clotehs_pred, zero_division=0))\n",
    "    print(mean_absolute_error(y_test, clotehs_pred))\n",
    "    print(mean_squared_error(y_test, clotehs_pred))\n",
    "\n",
    "    print(\"| face |\")\n",
    "    face_pred = model_face.predict(X_face_test)\n",
    "    log_face_pred = log_reg_face.predict(face_pred.reshape(-1, 1))\n",
    "\n",
    "    print(classification_report(y_face_test, log_face_pred, zero_division=0))\n",
    "    print(mean_absolute_error(y_face_test, face_pred))\n",
    "    print(mean_squared_error(y_face_test, face_pred))\n",
    "\n",
    "    plt.plot(y_test, clotehs_pred, 'o')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(y_test, skel_pred, 'o')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(y_face_test, face_pred, 'o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Pipeline([(\"poly\", PolynomialFeatures(degree=4)),(\"LinReg\", SVC())])\n",
    "m2 = Pipeline([(\"poly\", PolynomialFeatures(degree=4)),(\"LinReg\", SVC())])\n",
    "m3 = Pipeline([(\"poly\", PolynomialFeatures(degree=4)),(\"LinReg\", SVC())])\n",
    "\n",
    "test_regression(\"Pipeline\", m1, m2, m3, X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test, X_skel_logistic, X_clothes_logistic, X_face_logistic, X_skel_test_logistic, X_clothes_test_logistic, X_face_test_logistic, y_train_logistic, y_test_logistic, y_face_logistic, y_face_test_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"Regression\", SVC())])\n",
    "m2 = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"Regression\", SVC())])\n",
    "m3 = Pipeline([(\"poly\", PolynomialFeatures()), (\"scaler\", StandardScaler()), (\"Regression\", SVC())])\n",
    "test_regression(\"Pipeline\", m1, m2, m3, X_skel, X_clothes, X_face, X_skel_test, X_clothes_test, X_face_test, y_train, y_test, y_face, y_face_test, X_skel_logistic, X_clothes_logistic, X_face_logistic, X_skel_test_logistic, X_clothes_test_logistic, X_face_test_logistic, y_train_logistic, y_test_logistic, y_face_logistic, y_face_test_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import models, layers, Sequential, metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(units=9, input_shape=(9,)))\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Dense(units=9))\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Dense(units=2))\n",
    "model.add(layers.ReLU())\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "opt = keras.optimizers.SGD()\n",
    "opt.learning_rate.assign(0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy', 'mse'])\n",
    "\n",
    "history = model.fit(X_skel, y_train, epochs=100, \n",
    "                    validation_data=(X_skel_test, y_test))\n",
    "\n",
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('Confidence')\n",
    "# plt.ylim([0.5, 1])\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "test_acc = model.evaluate(X_skel_test,  y_test)\n",
    "print(test_acc)\n",
    "\n",
    "print(\"| face |\")\n",
    "face_pred = model.predict(X_skel_test)\n",
    "\n",
    "res = []\n",
    "for i in face_pred:\n",
    "    if i[0] > i[1]:\n",
    "        res.append(i[0])\n",
    "    else:\n",
    "        res.append(i[1])\n",
    "\n",
    "\n",
    "plt.plot(y_test, res, 'o')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we did was to read the dataset and extract the information that we need in order to recognize people:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b369d622223678a177c59fe4340d24ef9e31d9a4f6220fb3cdc831e5545a261b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
